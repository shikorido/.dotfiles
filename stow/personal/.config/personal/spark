#!/usr/bin/env sh

# Only for hadoop user
if [ "`id -un`" = hadoop ]; then
    # Sanity check
    ! [ -d ~/spark-3.5.3 ] && echo "$HOME/spark-3.5.3 is not a directory or does not exist." && return

    export SPARK_HOME=~/spark-3.5.3
    addToPathFront "$SPARK_HOME/bin"
    addToPathFront "$SPARK_HOME/sbin"
    export PYSPARK_PYTHON=python3

    if [[ :$PYTHONPATH: != *:$SPARK_HOME/python:* ]]; then
        export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
    fi

    # This function will run pyspark like jyputer notebook
    # If the two local PYTHON_DRIVER variables were global then pyspark shell
    # will not run itself
    #snotebook() {
    #    #SPARK_PATH=$SPARK_HOME
    #    local PYSPARK_DRIVER_PYTHON="jupyter"
    #    local PYSPARK_DRIVER_PYTHON_OPTS="notebook"
    #
    #    # local[2] means 2 working threads
    #    # local[*] means as many worker threads as logical cores on machine
    #    # zsh has special meaning under [], so it was quoted
    #    $SPARK_HOME/bin/pyspark --master 'local[2]'
    #}
fi

